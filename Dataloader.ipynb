{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spkit as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import mne\n",
    "from copy import deepcopy\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne_connectivity import envelope_correlation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from mne.preprocessing import ICA, corrmap, create_ecg_epochs, create_eog_epochs\n",
    "import autoreject\n",
    "from autoreject import AutoReject\n",
    "from autoreject import get_rejection_threshold\n",
    "from autoreject import Ransac\n",
    "from mne.preprocessing import annotate_amplitude\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPG_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.filepaths, self.labels = self._load_filepaths_and_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.filepaths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        eeg_data = self._load_eeg(filepath)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            eeg_data = self.transform(eeg_data)\n",
    "\n",
    "        return torch.tensor(eeg_data), torch.tensor(label)\n",
    "\n",
    "    def _load_filepaths_and_labels(self):\n",
    "        filepaths = []\n",
    "        labels = []\n",
    "\n",
    "        classes = sorted(os.listdir(self.root_dir))\n",
    "        for class_index, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                filenames = os.listdir(class_dir)\n",
    "                for filename in filenames:\n",
    "                    filepaths.append(os.path.join(class_dir, filename))\n",
    "                    labels.append(class_index)\n",
    "\n",
    "        return filepaths, labels\n",
    "\n",
    "    def _load_eeg(self, filepath):\n",
    "        data = mne.read_epochs(filepath, preload=False).get_data(picks='eeg');\n",
    "        normals = []\n",
    "        scaler = StandardScaler()\n",
    "        for idx in range(len(data)):\n",
    "            normals.append(scaler.fit_transform(data[idx]))\n",
    "\n",
    "        return np.array(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Concatenate data samples along the first dimension (window_count)\n",
    "    # Assumes each sample is a tuple (uid, data_sample)\n",
    "    uids, data_samples = zip(*batch)\n",
    "    concatenated_data = torch.stack(data_samples, dim=0)\n",
    "    return uids, concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FPG_Dataset(root_dir=r\"D:\\20second_MNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # Set your desired batch size\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    print(batch_idx)\n",
    "    print(targets)\n",
    "    for sample in inputs:\n",
    "        print(np.shape(sample.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
