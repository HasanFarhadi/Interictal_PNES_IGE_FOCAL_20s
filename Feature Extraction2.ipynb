{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spkit as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import mne\n",
    "from copy import deepcopy\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne_connectivity import envelope_correlation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from mne.preprocessing import ICA, corrmap, create_ecg_epochs, create_eog_epochs\n",
    "import autoreject\n",
    "from autoreject import AutoReject\n",
    "from autoreject import get_rejection_threshold\n",
    "from autoreject import Ransac\n",
    "from mne.preprocessing import annotate_amplitude\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "import networkx as nx\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import tsfresh\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "from pyrqa.time_series import TimeSeries\n",
    "from pyrqa.settings import Settings\n",
    "from pyrqa.analysis_type import Classic\n",
    "from pyrqa.neighbourhood import FixedRadius\n",
    "from pyrqa.metric import EuclideanMetric\n",
    "from pyrqa.computation import RQAComputation\n",
    "import pyrqa\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPG_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.filepaths, self.labels = self._load_filepaths_and_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.filepaths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        eeg_data = self._load_eeg(filepath)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            eeg_data = self.transform(eeg_data)\n",
    "\n",
    "        return torch.tensor(eeg_data), torch.tensor(label)\n",
    "\n",
    "    def _load_filepaths_and_labels(self):\n",
    "        filepaths = []\n",
    "        labels = []\n",
    "\n",
    "        classes = sorted(os.listdir(self.root_dir))\n",
    "        for class_index, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                filenames = os.listdir(class_dir)\n",
    "                for filename in filenames:\n",
    "                    filepaths.append(os.path.join(class_dir, filename))\n",
    "                    labels.append(class_index)\n",
    "\n",
    "        return filepaths, labels\n",
    "\n",
    "    def _load_eeg(self, filepath):\n",
    "        data = mne.read_epochs(filepath, preload=False).get_data(picks='eeg');\n",
    "        normals = []\n",
    "        scaler = StandardScaler()\n",
    "        for idx in range(len(data)):\n",
    "            normals.append(scaler.fit_transform(data[idx]))\n",
    "\n",
    "        return np.array(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Concatenate data samples along the first dimension (window_count)\n",
    "    # Assumes each sample is a tuple (uid, data_sample)\n",
    "    uids, data_samples = zip(*batch)\n",
    "    concatenated_data = torch.stack(data_samples, dim=0)\n",
    "    return uids, concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLV(sample, verbose=False):\n",
    "        \n",
    "        EEG_data = sample[0]\n",
    "        threshold = 0.3\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes (brain regions)\n",
    "        G.add_nodes_from(range(21))  # Assuming 21 brain regions\n",
    "\n",
    "        # Add edges (based on functional connectivity)\n",
    "        for i in range(21):\n",
    "            for j in range(i + 1, 21):\n",
    "                # Calculate functional connectivity strength between nodes i and j\n",
    "                # (e.g., using correlation coefficients from EEG signals)\n",
    "\n",
    "                #phase locking value\n",
    "                connectivity_strength = np.abs(np.mean(np.exp(1j * np.angle(EEG_data[i] * np.conj(EEG_data[j])))))  # Replace with actual method\n",
    "\n",
    "                if connectivity_strength > threshold:\n",
    "                    G.add_edge(i, j, weight=connectivity_strength)\n",
    "\n",
    "        # Calculate graph metrics\n",
    "        local_efficiency = nx.local_efficiency(G)\n",
    "        global_efficiency = nx.global_efficiency(G)\n",
    "\n",
    "        if (verbose):\n",
    "                \n",
    "            print(f\"Local efficiency: {local_efficiency}\")\n",
    "            print(f\"Global efficiency: {global_efficiency}\")\n",
    "\n",
    "        return local_efficiency, global_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RQA(data_points, dimension):\n",
    "    time_series = TimeSeries(data_points,\n",
    "                         embedding_dimension = int(dimension),\n",
    "                         time_delay =2)\n",
    "    settings = Settings(time_series,\n",
    "                    analysis_type=Classic,\n",
    "                    neighbourhood=FixedRadius(0.1),\n",
    "                    similarity_measure=EuclideanMetric,\n",
    "                    theiler_corrector=1)\n",
    "    computation = RQAComputation.create(settings,\n",
    "                                    verbose=False)\n",
    "    result = computation.run()\n",
    "    rqa_features = [\n",
    "        result.recurrence_rate,\n",
    "        result.determinism,\n",
    "        result.divergence,\n",
    "        result.entropy_diagonal_lines,\n",
    "        result.laminarity,\n",
    "        result.trapping_time,\n",
    "        result.entropy_vertical_lines,\n",
    "        result.entropy_white_vertical_lines,\n",
    "    ]\n",
    "    \n",
    "    return np.array(rqa_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eeg_features(eeg_data, include_rqa = False):\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    num_channels = eeg_data.shape[0]\n",
    "\n",
    "    # Initialize arrays to store features\n",
    "    mean_values = np.zeros(num_channels)\n",
    "    std_values = np.zeros(num_channels)\n",
    "    min_values = np.zeros(num_channels)\n",
    "    max_values = np.zeros(num_channels)\n",
    "    skewness_values = np.zeros(num_channels)\n",
    "    kurtosis_values = np.zeros(num_channels)\n",
    "    p2p_values = np.zeros(num_channels)\n",
    "    p2rms_values = np.zeros(num_channels)\n",
    "    rss_values = np.zeros(num_channels)\n",
    "    amplitude = np.zeros(num_channels)\n",
    "    rms = np.zeros(num_channels)\n",
    "    zero_crossing_rate = np.zeros(num_channels)\n",
    "    delta_power = np.zeros(num_channels)\n",
    "    theta_power = np.zeros(num_channels)\n",
    "    alpha_power = np.zeros(num_channels)\n",
    "    beta_power = np.zeros(num_channels)\n",
    "    gamma_power = np.zeros(num_channels)\n",
    "    spectral_entropy = np.zeros(num_channels)\n",
    "    rqa_matrix = []\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        # Extract data for the current channel\n",
    "        channel_data = eeg_data[channel]\n",
    "\n",
    "        # Compute mean, STD, min, max\n",
    "        mean_values[channel] = np.mean(channel_data)\n",
    "        std_values[channel] = np.std(channel_data)\n",
    "        min_values[channel] = np.min(channel_data)\n",
    "        max_values[channel] = np.max(channel_data)\n",
    "\n",
    "        # Compute skewness and kurtosis\n",
    "        skewness_values[channel] = skew(channel_data)\n",
    "        kurtosis_values[channel] = kurtosis(channel_data)\n",
    "\n",
    "        # Compute peak-to-peak (P2P)\n",
    "        p2p_values[channel] = max_values[channel] - min_values[channel]\n",
    "\n",
    "        # Compute peak-to-root sum square (P2RMS)\n",
    "        p2rms_values[channel] = np.sqrt(np.sum(channel_data**2))\n",
    "\n",
    "        # Compute root sum square (RSS)\n",
    "        rss_values[channel] = np.sqrt(np.sum(channel_data**2))\n",
    "\n",
    "        # Compute amplitude (peak-to-peak divided by 2)\n",
    "        amplitude[channel] = p2p_values[channel] / 2\n",
    "\n",
    "        # Compute root mean square (RMS)\n",
    "        rms[channel] = np.sqrt(np.mean(channel_data**2))\n",
    "\n",
    "        # Compute zero-crossing rate\n",
    "        zero_crossing_rate[channel] = np.mean(np.diff(np.sign(channel_data)) != 0)\n",
    "\n",
    "        # Compute power spectral density using Welch method\n",
    "        f, psd = welch(channel_data, fs=1000, nperseg=256)\n",
    "        delta_power[channel] = np.sum(psd[(f >= 1) & (f <= 4)])\n",
    "        theta_power[channel] = np.sum(psd[(f >= 4) & (f <= 8)])\n",
    "        alpha_power[channel] = np.sum(psd[(f >= 8) & (f <= 14)])\n",
    "        beta_power[channel] = np.sum(psd[(f >= 14) & (f <= 30)])\n",
    "        gamma_power[channel] = np.sum(psd[f > 30])\n",
    "        if(include_rqa):\n",
    "            rqa_matrix.append(RQA(channel_data, dimension=4))\n",
    "\n",
    "        # Compute spectral entropy\n",
    "        spectral_entropy[channel] = -np.sum(psd * np.log2(psd))\n",
    "\n",
    "    # Organize features into a dictionary\n",
    "    features = np.array([mean_values,std_values,min_values,max_values,skewness_values,kurtosis_values,p2p_values,p2rms_values, rss_values,amplitude, rms, zero_crossing_rate,delta_power, theta_power, alpha_power,beta_power,gamma_power,spectral_entropy]).T\n",
    "    if(include_rqa):\n",
    "        features = np.append(features, np.array(rqa_matrix), axis=1)\n",
    "    return features\n",
    "\n",
    "#extracted_features = extract_eeg_features(sample[0])\n",
    "#extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(path, verbose = False):\n",
    "    dataset = FPG_Dataset(root_dir=path)\n",
    "\n",
    "    batch_size = 1  # Set your desired batch size\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    label = []\n",
    "    feature_space = []\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        for patient in inputs:\n",
    "            \n",
    "            for epoch in patient:\n",
    "                feature_space.append(extract_eeg_features(epoch))\n",
    "                label.append(targets)\n",
    "            \n",
    "            if(verbose):\n",
    "                print(\"Patient processed\")\n",
    "    \n",
    "    feature_space = np.squeeze(feature_space)\n",
    "    labels = np.array([tensor.numpy() for tensor in label])\n",
    "    flat_features = feature_space.reshape(feature_space.shape[0], -1)\n",
    "\n",
    " \n",
    "    return feature_space, labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = FPG_Dataset(root_dir=r\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67 (+/- 0.28)\n"
     ]
    }
   ],
   "source": [
    "csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "clf = Pipeline([('CSP', csp), ('LDA', LinearDiscriminantAnalysis())])\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=5, scoring='accuracy')\n",
    "\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11912\\3707840691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoad_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11912\\2382347244.py\u001b[0m in \u001b[0;36mLoad_Data\u001b[1;34m(path, verbose)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpatient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mfeature_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_eeg_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11912\\1255805577.py\u001b[0m in \u001b[0;36mextract_eeg_features\u001b[1;34m(eeg_data, include_rqa)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Compute skewness and kurtosis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mskewness_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mkurtosis_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkurtosis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py\u001b[0m in \u001b[0;36maxis_nan_policy_wrapper\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    476\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                     contains_nan, _ = (\n\u001b[1;32m--> 478\u001b[1;33m                         scipy.stats._stats_py._contains_nan(sample, nan_policy))\n\u001b[0m\u001b[0;32m    479\u001b[0m                     \u001b[0mcontains_nans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontains_nan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py\u001b[0m in \u001b[0;36m_contains_nan\u001b[1;34m(a, nan_policy, use_summation)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_summation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                 \u001b[0mcontains_nan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mcontains_nan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2324\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2325\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train = Load_Data(r\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n",
      "Patient processed\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = Load_Data(r\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS_TEST\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart = time.time()\\n\\nrf_classifier = RandomForestClassifier(n_estimators=80, random_state=42)\\nknn_classifier = KNeighborsClassifier(n_neighbors=100)\\nsvm_classifier = SVC()\\novr_classifier = OneVsRestClassifier(rf_classifier)\\novr_classifier.fit(X_train, y_train)\\ny_pred = ovr_classifier.predict(X_test)\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f\"Accuracy: {accuracy:.2f}\")\\nprint(f\"training time: {time.time() - start}\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=80, random_state=42)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=100)\n",
    "svm_classifier = SVC()\n",
    "ovr_classifier = OneVsRestClassifier(rf_classifier)\n",
    "ovr_classifier.fit(X_train, y_train)\n",
    "y_pred = ovr_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"training time: {time.time() - start}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
