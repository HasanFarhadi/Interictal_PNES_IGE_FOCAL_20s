{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spkit as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import mne\n",
    "from copy import deepcopy\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne_connectivity import envelope_correlation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from mne.preprocessing import ICA, corrmap, create_ecg_epochs, create_eog_epochs\n",
    "import autoreject\n",
    "from autoreject import AutoReject\n",
    "from autoreject import get_rejection_threshold\n",
    "from autoreject import Ransac\n",
    "from mne.preprocessing import annotate_amplitude\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "import networkx as nx\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import tsfresh\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "from pyrqa.time_series import TimeSeries\n",
    "from pyrqa.settings import Settings\n",
    "from pyrqa.analysis_type import Classic\n",
    "from pyrqa.neighbourhood import FixedRadius\n",
    "from pyrqa.metric import EuclideanMetric\n",
    "from pyrqa.computation import RQAComputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPG_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.filepaths, self.labels = self._load_filepaths_and_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.filepaths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        eeg_data = self._load_eeg(filepath)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            eeg_data = self.transform(eeg_data)\n",
    "\n",
    "        return torch.tensor(eeg_data), torch.tensor(label)\n",
    "\n",
    "    def _load_filepaths_and_labels(self):\n",
    "        filepaths = []\n",
    "        labels = []\n",
    "\n",
    "        classes = sorted(os.listdir(self.root_dir))\n",
    "        for class_index, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                filenames = os.listdir(class_dir)\n",
    "                for filename in filenames:\n",
    "                    filepaths.append(os.path.join(class_dir, filename))\n",
    "                    labels.append(class_index)\n",
    "\n",
    "        return filepaths, labels\n",
    "\n",
    "    def _load_eeg(self, filepath):\n",
    "        data = mne.read_epochs(filepath, preload=False).get_data(picks='eeg');\n",
    "        normals = []\n",
    "        scaler = StandardScaler()\n",
    "        for idx in range(len(data)):\n",
    "            normals.append(scaler.fit_transform(data[idx]))\n",
    "\n",
    "        return np.array(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Concatenate data samples along the first dimension (window_count)\n",
    "    # Assumes each sample is a tuple (uid, data_sample)\n",
    "    uids, data_samples = zip(*batch)\n",
    "    concatenated_data = torch.stack(data_samples, dim=0)\n",
    "    return uids, concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLV(sample, verbose=False):\n",
    "        \n",
    "        EEG_data = sample[0]\n",
    "        threshold = 0.3\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes (brain regions)\n",
    "        G.add_nodes_from(range(21))  # Assuming 21 brain regions\n",
    "\n",
    "        # Add edges (based on functional connectivity)\n",
    "        for i in range(21):\n",
    "            for j in range(i + 1, 21):\n",
    "                # Calculate functional connectivity strength between nodes i and j\n",
    "                # (e.g., using correlation coefficients from EEG signals)\n",
    "\n",
    "                #phase locking value\n",
    "                connectivity_strength = np.abs(np.mean(np.exp(1j * np.angle(EEG_data[i] * np.conj(EEG_data[j])))))  # Replace with actual method\n",
    "\n",
    "                if connectivity_strength > threshold:\n",
    "                    G.add_edge(i, j, weight=connectivity_strength)\n",
    "\n",
    "        # Calculate graph metrics\n",
    "        local_efficiency = nx.local_efficiency(G)\n",
    "        global_efficiency = nx.global_efficiency(G)\n",
    "\n",
    "        if (verbose):\n",
    "                \n",
    "            print(f\"Local efficiency: {local_efficiency}\")\n",
    "            print(f\"Global efficiency: {global_efficiency}\")\n",
    "\n",
    "        return local_efficiency, global_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eeg_features(eeg_data):\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    num_channels = eeg_data.shape[0]\n",
    "\n",
    "    # Initialize arrays to store features\n",
    "    mean_values = np.zeros(num_channels)\n",
    "    std_values = np.zeros(num_channels)\n",
    "    min_values = np.zeros(num_channels)\n",
    "    max_values = np.zeros(num_channels)\n",
    "    skewness_values = np.zeros(num_channels)\n",
    "    kurtosis_values = np.zeros(num_channels)\n",
    "    p2p_values = np.zeros(num_channels)\n",
    "    p2rms_values = np.zeros(num_channels)\n",
    "    rss_values = np.zeros(num_channels)\n",
    "    amplitude = np.zeros(num_channels)\n",
    "    rms = np.zeros(num_channels)\n",
    "    zero_crossing_rate = np.zeros(num_channels)\n",
    "    delta_power = np.zeros(num_channels)\n",
    "    theta_power = np.zeros(num_channels)\n",
    "    alpha_power = np.zeros(num_channels)\n",
    "    beta_power = np.zeros(num_channels)\n",
    "    gamma_power = np.zeros(num_channels)\n",
    "    spectral_entropy = np.zeros(num_channels)\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        # Extract data for the current channel\n",
    "        channel_data = eeg_data[channel]\n",
    "\n",
    "        # Compute mean, STD, min, max\n",
    "        mean_values[channel] = np.mean(channel_data)\n",
    "        std_values[channel] = np.std(channel_data)\n",
    "        min_values[channel] = np.min(channel_data)\n",
    "        max_values[channel] = np.max(channel_data)\n",
    "\n",
    "        # Compute skewness and kurtosis\n",
    "        skewness_values[channel] = skew(channel_data)\n",
    "        kurtosis_values[channel] = kurtosis(channel_data)\n",
    "\n",
    "        # Compute peak-to-peak (P2P)\n",
    "        p2p_values[channel] = max_values[channel] - min_values[channel]\n",
    "\n",
    "        # Compute peak-to-root sum square (P2RMS)\n",
    "        p2rms_values[channel] = np.sqrt(np.sum(channel_data**2))\n",
    "\n",
    "        # Compute root sum square (RSS)\n",
    "        rss_values[channel] = np.sqrt(np.sum(channel_data**2))\n",
    "\n",
    "        # Compute amplitude (peak-to-peak divided by 2)\n",
    "        amplitude[channel] = p2p_values[channel] / 2\n",
    "\n",
    "        # Compute root mean square (RMS)\n",
    "        rms[channel] = np.sqrt(np.mean(channel_data**2))\n",
    "\n",
    "        # Compute zero-crossing rate\n",
    "        zero_crossing_rate[channel] = np.mean(np.diff(np.sign(channel_data)) != 0)\n",
    "\n",
    "        # Compute power spectral density using Welch method\n",
    "        f, psd = welch(channel_data, fs=1000, nperseg=256)\n",
    "        delta_power[channel] = np.sum(psd[(f >= 1) & (f <= 4)])\n",
    "        theta_power[channel] = np.sum(psd[(f >= 4) & (f <= 8)])\n",
    "        alpha_power[channel] = np.sum(psd[(f >= 8) & (f <= 14)])\n",
    "        beta_power[channel] = np.sum(psd[(f >= 14) & (f <= 30)])\n",
    "        gamma_power[channel] = np.sum(psd[f > 30])\n",
    "\n",
    "        # Compute spectral entropy\n",
    "        spectral_entropy[channel] = -np.sum(psd * np.log2(psd))\n",
    "\n",
    "    # Organize features into a dictionary\n",
    "    features = np.array([mean_values,std_values,min_values,max_values,skewness_values,kurtosis_values,p2p_values,p2rms_values, rss_values,amplitude, rms, zero_crossing_rate,delta_power, theta_power, alpha_power,beta_power,gamma_power,spectral_entropy])\n",
    "\n",
    "    return features.T\n",
    "\n",
    "#extracted_features = extract_eeg_features(sample[0])\n",
    "#extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(path, verbose = False):\n",
    "    dataset = FPG_Dataset(root_dir=path)\n",
    "\n",
    "    batch_size = 1  # Set your desired batch size\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    label = []\n",
    "    feature_space = []\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        for patient in inputs:\n",
    "            \n",
    "            for epoch in patient:\n",
    "                feature_space.append(extract_eeg_features(epoch))\n",
    "                label.append(targets)\n",
    "    \n",
    "    feature_space = np.squeeze(feature_space)\n",
    "    labels = np.array([tensor.numpy() for tensor in label])\n",
    "    flat_features = feature_space.reshape(feature_space.shape[0], -1)\n",
    "\n",
    "    if (verbose):\n",
    "        print(f\"X size: {flat_features.shape}\")\n",
    "        print(f\"y size: {labels.shape}\")\n",
    "    return flat_features, labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = FPG_Dataset(root_dir=r\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQA Result:\n",
      "===========\n",
      "\n",
      "Minimum diagonal line length (L_min): 2\n",
      "Minimum vertical line length (V_min): 2\n",
      "Minimum white vertical line length (W_min): 2\n",
      "\n",
      "Recurrence rate (RR): 0.248118\n",
      "Determinism (DET): 0.996689\n",
      "Average diagonal line length (L): 10.395985\n",
      "Longest diagonal line length (L_max): 6297\n",
      "Divergence (DIV): 0.000159\n",
      "Entropy diagonal lines (L_entr): 3.169949\n",
      "Laminarity (LAM): 0.998668\n",
      "Trapping time (TT): 14.855438\n",
      "Longest vertical line length (V_max): 95\n",
      "Entropy vertical lines (V_entr): 3.571348\n",
      "Average white vertical line length (W): 44.713057\n",
      "Longest white vertical line length (W_max): 2410\n",
      "Longest white vertical line length inverse (W_div): 0.000415\n",
      "Entropy white vertical lines (W_entr): 4.704160\n",
      "\n",
      "Ratio determinism / recurrence rate (DET/RR): 4.016994\n",
      "Ratio laminarity / determinism (LAM/DET): 1.001986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = Dataset.__getitem__(1)[0][110]\n",
    "sample.shape\n",
    "data_points = sample[0, :]\n",
    "time_series = TimeSeries(data_points,\n",
    "                         embedding_dimension=2,\n",
    "                         time_delay=2)\n",
    "settings = Settings(time_series,\n",
    "                    analysis_type=Classic,\n",
    "                    neighbourhood=FixedRadius(0.65),\n",
    "                    similarity_measure=EuclideanMetric,\n",
    "                    theiler_corrector=1)\n",
    "computation = RQAComputation.create(settings,\n",
    "                                    verbose=False)\n",
    "\n",
    "result = computation.run()\n",
    "result.min_diagonal_line_length = 2\n",
    "result.min_vertical_line_length = 2\n",
    "result.min_white_vertical_line_length = 2\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = Load_Data(r\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, y_test = Load_Data(r\"C:\\Users\\admin\\Desktop\\20second_MNE_3CLASS_TEST\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart = time.time()\\n\\nrf_classifier = RandomForestClassifier(n_estimators=80, random_state=42)\\nknn_classifier = KNeighborsClassifier(n_neighbors=100)\\nsvm_classifier = SVC()\\novr_classifier = OneVsRestClassifier(rf_classifier)\\novr_classifier.fit(X_train, y_train)\\ny_pred = ovr_classifier.predict(X_test)\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f\"Accuracy: {accuracy:.2f}\")\\nprint(f\"training time: {time.time() - start}\")\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=80, random_state=42)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=100)\n",
    "svm_classifier = SVC()\n",
    "ovr_classifier = OneVsRestClassifier(rf_classifier)\n",
    "ovr_classifier.fit(X_train, y_train)\n",
    "y_pred = ovr_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"training time: {time.time() - start}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
